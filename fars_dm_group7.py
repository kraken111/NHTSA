# -*- coding: utf-8 -*-
"""FARS_DM_Group7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gEuQBdS9MsY63cBq6jvYIrk72jLZpnZ4

##Import Packages
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import itertools
import pandas as pd
import numpy as np
import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
import xgboost as xgb
from sklearn.externals import joblib
import matplotlib
import graphviz
import matplotlib.pyplot as plt
import time
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import confusion_matrix
import seaborn as sns

"""##Data Exploration"""

df = pd.read_csv("/content/fars_train_proc.csv")
df.head()

df.describe()

#check for null values
df.isnull().sum()

"""##Feature Engineering"""

#check classes balance
df['INJURY_SEVERITY'].value_counts()

#class merge and drop
df.INJURY_SEVERITY.replace(['Unknown', 'Injured_Severity_Unknown'], 'Possible_Injury', inplace=True)
#merge classes
df = df[df.INJURY_SEVERITY != 'Died_Prior_to_Accident'] # drop class

#check the number of distinct occurences for values in a column
df['SEX'].value_counts() # Memo:  SEX, ALCOHOL_TEST_RESULT, DRUG_TEST_RESULTS_(1_of_3), HISPANIC_ORIGIN, RACE

#estimate correlation between a certain category value and injury severity
category = 'SEX'
value = 'Unknown'
dcorr = df.loc[df[category] == value]

print('Injury severity distribution for ' + category + " - " + str(value))
dcorr['INJURY_SEVERITY'].value_counts()

"""##Compressing the data from polics reports, test types and test results"""

#Counting how many drug tests were conducted
df['DRUG_TEST_COUNT'] = (df[['DRUG_TEST_RESULTS_(1_of_3)','DRUG_TEST_RESULTS_(2_of_3)','DRUG_TEST_RESULTS_(3_of_3)']] != 0).sum(axis=1)
#Averaging test results only excluding zeros by replacing them with NaN for the calculation
df['DRUG_TEST_RESULTS_AVG'] = (df[['DRUG_TEST_RESULTS_(1_of_3)','DRUG_TEST_RESULTS_(2_of_3)','DRUG_TEST_RESULTS_(3_of_3)']].replace(0, np.NaN).mean(axis=1))
#Returning NaN to Zeros
df['DRUG_TEST_RESULTS_AVG'].fillna(0, inplace=True)

#Drop processed columns
df.drop(columns=['DRUG_TEST_RESULTS_(1_of_3)','DRUG_TEST_RESULTS_(2_of_3)','DRUG_TEST_RESULTS_(3_of_3)',
                 'POLICE-REPORTED_DRUG_INVOLVEMENT','METHOD_OF_DRUG_DETERMINATION','DRUG_TEST_TYPE',
                 'DRUG_TEST_TYPE_(2_of_3)','DRUG_TEST_TYPE_(3_of_3)','POLICE_REPORTED_ALCOHOL_INVOLVEMENT',
                 'METHOD_ALCOHOL_DETERMINATION'], inplace=True)

df_ethnic = df[['HISPANIC_ORIGIN','RACE']]
df_ethnic.head(50)
#df.loc[df_ethnic['HISPANIC_ORIGIN']== 'Central_or_South_American']

ethnics = df.HISPANIC_ORIGIN.unique()
#ethnics #List all the possible categories under 'HISPANIC_ORIGIN'
#keeping only non-negative categories
ethnics = [E for E in ethnics if E not in ('Not_a_Fatality_(Not_Applicable)','Unknown','Non-Hispanic')]
#ethnics
df['HISPANIC_ORIGIN'] = df['HISPANIC_ORIGIN'].replace(ethnics, 'Hispanic')

RACE_ETHNICITY_zip = zip(list(df['HISPANIC_ORIGIN']), list(df['RACE']))
RACE_ETHNICITY = []
for tpl in RACE_ETHNICITY_zip:
  if tpl[0] == 'Hispanic':
    RACE_ETHNICITY.append(tpl[0])
  else:
    RACE_ETHNICITY.append(tpl[1])

df['RACE_ETHNICITY'] = RACE_ETHNICITY #create the new column
df.drop(columns=['HISPANIC_ORIGIN', 'RACE'], inplace=True) #Drop the merged columns

"""##Related Factor number - PERSON_LEVEL"""

#Check number of distinct occurences for the values in a column
df['RELATED_FACTOR_(1)-PERSON_LEVEL'].value_counts()

#Estimate correlation between a certain category value and injury severity
category = 'RELATED_FACTOR_(1)-PERSON_LEVEL'
value = 'Improper_Crossing_or_Roadway_or_Intersection'
dcorr = df.loc[df[category] == value] # for specific category

print('Injury severity distribution for ' + category + " - " + str(value))
dcorr['INJURY_SEVERITY'].value_counts()

"""They are rarely used, but some convey strong information about the severity of injury as shown above, and I don't want to lose this information. I will approach this problem by one-hot-encoding the categories and then merging duplicates, but first I need to prepare the data for one-hot-encoding.

##Thresholding Values in Columns by Counts
A recurrent problem with this dataset are rare categories; one-hot-encoding will create a new feature for every rare category, and this will create many redundant parameters. I will deal with this by setting a count threshold. Categories that are counted more times than the threshold will be kept as is, others will be labeled as 'rare'. The count threshold will be a hyperparamter for this model as it is hard to predict what will be the best value for it.
"""

#Deciding the initial threshold to be 1% of the dataset size
tot_instances = df.shape[0]
threshold = tot_instances*0.005
print('The minimum count threshold is: '+str(threshold))

#Apply the count threshold to all the categorical values
obj_columns = list(df.select_dtypes(include=['object']).columns) #list of all the column names with object dtype
obj_columns.remove('INJURY_SEVERITY') # to keep rare class
df = df.apply(lambda x: x.mask(x.map(x.value_counts())<threshold, 'RARE') if x.name in obj_columns else x)

"""##One Hot Encoding"""

#One Hot Encode the categorical features in the dataset
df_encoded = pd.get_dummies(data=df, columns=obj_columns)
df_encoded.dtypes

# Merge dulpicates from RELATED_FACTOR_(#)-PERSON_LEVEL
df_encoded['RELATED_FACTOR_RARE'] = df_encoded['RELATED_FACTOR_(1)-PERSON_LEVEL_RARE'] + df_encoded['RELATED_FACTOR_(2)-PERSON_LEVEL_RARE'] + df_encoded['RELATED_FACTOR_(3)-PERSON_LEVEL_RARE']
df_encoded['RELATED_FACTOR_Not_Applicable'] = df_encoded['RELATED_FACTOR_(1)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons'] + df_encoded['RELATED_FACTOR_(2)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons'] + df_encoded['RELATED_FACTOR_(3)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons']
#Drop the merged columns
df_encoded.drop(columns=['RELATED_FACTOR_(1)-PERSON_LEVEL_RARE',
                 'RELATED_FACTOR_(2)-PERSON_LEVEL_RARE',
                 'RELATED_FACTOR_(3)-PERSON_LEVEL_RARE', 
                 'RELATED_FACTOR_(1)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons',
                 'RELATED_FACTOR_(2)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons',
                 'RELATED_FACTOR_(3)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons'], inplace=True)

#Validate the new features
df_encoded.head(50)

#Split the dataset to X and Y for model fit
X_Orig = df_encoded.dropna()
Y_Orig = X_Orig['INJURY_SEVERITY'] #Set the target column
X_Orig = X_Orig.drop('INJURY_SEVERITY', axis=1) #Remove the target column for the training data
X_test = df_encoded[df_encoded['INJURY_SEVERITY'].isnull()]
X_test = X_test.drop('INJURY_SEVERITY', axis=1)

#Encode the target classes to specific requested numbers
class_dict = {'Possible_Injury': 0, 'No_Injury': 1, 'Fatal_Injury': 2,
              'Nonincapaciting_Evident_Injury': 3, 'Incapaciting_Injury': 4}
Y_Orig = pd.DataFrame(Y_Orig)
Y_encoded = Y_Orig.replace({'INJURY_SEVERITY' : class_dict})

#Split the data set to train and dev sets
X_train, X_dev, Y_train, Y_dev = train_test_split(X_Orig, Y_encoded, test_size=0.3, random_state=42)
#Note: train_test_split() already shuffles the dataset

"""##Fitting the Model"""

#Convert that data to the XGBoost specific DMatrix data format
Dtrain = xgb.DMatrix(X_train, label=Y_train)
Ddev = xgb.DMatrix(X_dev, label=Y_dev)
Dcv = xgb.DMatrix(X_Orig, label=Y_encoded) # for cross-validation

#Set the parameters for the model
num_class  = len(np.unique(Y_train))
param = {                     # General guidelines for initial paramaters:
    'min_child_weight': 1,    # 1 (choose small for high class imbalance)
    'gamma': 0.5,
    'lambda': 10,             # L2 Regulariztion - default = 1
    'scale_pos_weight': 1,    # 1 (choose small for high class imbalance)
    'subsample': 0.6,         # 0.5-0.9
    'colsample_bytree': 0.8,  # 0.5-0.9 
    'colsample_bylevel': 0.7, # 0.5-0.9
    'max_depth': 5,           # 3-10 
    'eta': 0.1,               # 0.05-0.3
    'silent': 0,              # 0 - prints progress    1 - quiet
    'objective': 'multi:softmax',
    'num_class': num_class,
    'eval_metric': 'mlogloss'}
num_round = 1000 # the number of training iterations if not stopped early
evallist = [(Dtrain, 'train'), (Ddev, 'eval')] # Specify validation set to watch performance

#Train the model on the training set to get an intial impression on the performance
model = xgb.train(param, Dtrain, num_round, evallist, early_stopping_rounds=10)
print("Best error: {:.2f} with {} rounds".format(
    model.best_score,
    model.best_iteration+1
))